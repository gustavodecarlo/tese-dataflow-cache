apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: "covid-raw-{{ macros.uuid.uuid4() }}-{{ task_instance.try_number }}"
  namespace: default
spec:
  type: Python
  mode: cluster
  image: "tese-spark:latest"
  imagePullPolicy: IfNotPresent
  mainApplicationFile: "local:///opt/spark/work-dir/cenarios/cenario_od_covid/covid_pipelines/main.py"
  arguments: [
    "covid-raw",
    "--source-data",
    "gs://lncc-tese-datafrag/source_covid/csse_covid_19_daily_reports",
    "--table",
    "covid_raw"  
  ]
  hadoopConf:
    "google.cloud.auth.service.account.enable": "true"
    "google.cloud.auth.service.account.json.keyfile": "{{ env_config.google_application_credentials }}"
    "fs.gs.outputstream.buffer.size": "33554432"
    "fs.gs.outputstream.pipe.buffer.size": "8388608"
    "fs.gs.outputstream.upload.chunk.size": "134217728"
    "fs.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"
    "fs.AbstractFileSystem.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"
  sparkConf:
    "spark.sql.shuffle.partitions": "10"
    "spark.sql.parquet.compression.codec": "gzip"
  sparkVersion: "3.3.2"
  restartPolicy:
    type: Never
  volumes:
    - name: "test-volume"
      hostPath:
        path: "/tmp"
        type: Directory
  driver:
    cores: 2
    memory: "3000m"
    labels:
      version: 3.3.2
    serviceAccount: spark
    env:
    - name: WAREHOUSE
      value: "{{ env_config.warehouse }}"
    - name: CASSANDRA_HOST
      value: "{{ env_config.cassandra_host }}"
    - name: CASSANDRA_PORT
      value: "{{ env_config.cassandra_port }}"
    - name: GOOGLE_BUCKET
      value: "{{ env_config.google_bucket }}"
    - name: DATAFRAG_WAREHOUSE
      value: "{{ env_config.datafrag_warehouse }}"
    - name: DATAFRAG_METATABLE
      value: "{{ env_config.datafrag_metatable }}"
    - name: DATAFRAG_TC_METATABLE
      value: "{{ env_config.datafrag_tc_metatable }}"
    - name: DATAFRAG_KEYSPACE
      value: "{{ env_config.datafrag_keyspace }}"
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: "{{ env_config.google_application_credentials }}"
    volumeMounts:
      - name: "test-volume"
        mountPath: "/tmp"
  executor:
    cores: 1
    instances: 1
    memory: "1500m"
    labels:
      version: 3.3.2
    serviceAccount: spark
    env:
    - name: WAREHOUSE
      value: "{{ env_config.warehouse }}"
    - name: CASSANDRA_HOST
      value: "{{ env_config.cassandra_host }}"
    - name: CASSANDRA_PORT
      value: "{{ env_config.cassandra_port }}"
    - name: GOOGLE_BUCKET
      value: "{{ env_config.google_bucket }}"
    - name: DATAFRAG_WAREHOUSE
      value: "{{ env_config.datafrag_warehouse }}"
    - name: DATAFRAG_METATABLE
      value: "{{ env_config.datafrag_metatable }}"
    - name: DATAFRAG_TC_METATABLE
      value: "{{ env_config.datafrag_tc_metatable }}"
    - name: DATAFRAG_KEYSPACE
      value: "{{ env_config.datafrag_keyspace }}"
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: "{{ env_config.google_application_credentials }}"
    volumeMounts:
      - name: "test-volume"
        mountPath: "/tmp"